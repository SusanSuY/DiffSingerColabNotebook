{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SusanSuY/DiffSingerColabNotebook/blob/main/Kei's_DiffSinger_colab_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start with an NNSVS database. Once you have that, train it starting and stopping at Stage 0. Save the `ETK/train/data/acoustic` folder."
      ],
      "metadata": {
        "id": "tpZBmXMEERBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "aAvdR13BIkCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Install NNSVS\n",
        "!git clone https://github.com/nnsvs/nnsvs.git\n",
        "%cd nnsvs\n",
        "!pip install -e \".[dev,lint,test]\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "Xsp0OxZlEuVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Patch nnsvs2opencpop.py\n",
        "%%writefile /content/nnsvs/utils/nnsvs2opencpop.py\n",
        "\"\"\"Convert NNSVS's segmented data to Opencpop's structure\n",
        "\n",
        "so that the code for DiffSinger can be used.\n",
        "\"\"\"\n",
        "import argparse\n",
        "import re\n",
        "import shutil\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import librosa\n",
        "from nnmnkwii.frontend import merlin as fe\n",
        "from nnmnkwii.io import hts\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "def note_by_regex(regex, s):\n",
        "    match = re.search(regex, s)\n",
        "    if match is None:\n",
        "        return 'rest'\n",
        "    return match.group(1)\n",
        "\n",
        "\n",
        "def numeric_feature_by_regex(regex, s):\n",
        "    match = re.search(regex, s)\n",
        "    if match is None:\n",
        "        return 0\n",
        "    return int(match.group(1))\n",
        "\n",
        "\n",
        "def get_parser():\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Convert NNSVS's segmented data to Opencpop's structure\",\n",
        "    )\n",
        "    parser.add_argument(\"in_dir\", type=str, help=\"Path to input dir\")\n",
        "    parser.add_argument(\"out_dir\", type=str, help=\"Output directory\")\n",
        "    return parser\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = get_parser().parse_args(sys.argv[1:])\n",
        "    in_dir = Path(args.in_dir)\n",
        "    out_dir = Path(args.out_dir)\n",
        "\n",
        "    label_score_dir = in_dir / \"label_phone_score\"\n",
        "    label_align_dir = in_dir / \"label_phone_align\"\n",
        "    in_wav_dir = in_dir / \"wav\"\n",
        "\n",
        "    out_wav_dir = out_dir / \"wavs\"\n",
        "    out_wav_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    label_score_files = sorted(label_score_dir.glob(\"*.lab\"))\n",
        "    utt_ids = [f.stem for f in label_score_files]\n",
        "\n",
        "    rows = []\n",
        "    for utt_id in tqdm(utt_ids):\n",
        "        if utt_id in [\"namine_ritsu_hana_seg12\"]:\n",
        "            continue\n",
        "        label_score = hts.load(label_score_dir / f\"{utt_id}.lab\")\n",
        "        label_align = hts.load(label_align_dir / f\"{utt_id}.lab\")\n",
        "\n",
        "        ph = [\n",
        "            re.search(r\"\\-(.*?)\\+\", context).group(1)\n",
        "            for context in label_score.contexts\n",
        "        ]\n",
        "        note = [\n",
        "            note_by_regex(r\"/E:([A-Z][b]?[0-9]+)]\", context)\n",
        "            for context in label_score.contexts\n",
        "        ]\n",
        "        note_dur = [\n",
        "            numeric_feature_by_regex(r\"@(\\d+)#\", context) / 100.0\n",
        "            for context in label_score.contexts\n",
        "        ]\n",
        "        ph_dur = fe.duration_features(label_align).reshape(-1) * 0.005\n",
        "        is_slur = [0] * len(ph_dur)\n",
        "        #assert len(ph) == len(note) == len(note_dur) == len(ph_dur) == len(is_slur)\n",
        "        cols = [\n",
        "            utt_id,\n",
        "            \" \".join(ph),\n",
        "            \" \".join(ph),\n",
        "            \" \".join(str(n) for n in note),\n",
        "            \" \".join(str(n) for n in note_dur),\n",
        "            \" \".join(str(round(n, 3)) for n in ph_dur),\n",
        "            \" \".join(str(n) for n in is_slur),\n",
        "        ]\n",
        "        rows.append(\"|\".join(cols))\n",
        "        shutil.copyfile(in_wav_dir / f\"{utt_id}.wav\", out_wav_dir / f\"{utt_id}.wav\")\n",
        "\n",
        "    with open(out_dir / \"transcriptions.txt\", \"w\") as f:\n",
        "        for row in rows:\n",
        "            f.write(row + \"\\n\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VRAMq_sTFBr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Convert your NNSVS dataset to Opencpop format\n",
        "#@markdown Upload the contents of your `ETK/train/data/acoustic` folder to this colab session, and copy+paste the directory of it into the following box, then run this cell.\n",
        "acoustic_data_path = ''#@param{type:'string'}\n",
        "!python utils/nnsvs2opencpop.py {acoustic_data_path} /content/segments"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2zgleqj4FRQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown # Alternative - Upload Opencpop formatted data\n",
        "# @markdown If you have already converted your NNSVS database to Opencpop format, you may upload it as a zip file to the session (or to Drive) and unpack it here.\n",
        "opencpop_data_path = ''#@param{type:'string'}\n",
        "!unzip {opencpop_data_path} -d /content/segments"
      ],
      "metadata": {
        "id": "rxocLfa25v0c",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pK8aicf8A2sj"
      },
      "outputs": [],
      "source": [
        "#@markdown # Install DiffSinger\n",
        "%cd /content\n",
        "!git clone https://github.com/openvpi/DiffSinger\n",
        "!pip install onnx==1.12.0 onnxsim==0.4.10 protobuf==3.13.0\n",
        "!pip3 install torch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1\n",
        "!wget https://github.com/openvpi/vocoders/releases/download/nsf-hifigan-v1/nsf_hifigan_20221211.zip\n",
        "!7za  -bso0 -y x /content/nsf_hifigan_20221211.zip -o/content/DiffSinger/checkpoints\n",
        "%cd DiffSinger\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take the folder called `segments` and drag it into the following directory, where {speaker_name} is the name of your singer: `/content/DiffSinger/data/{speaker_name}`"
      ],
      "metadata": {
        "id": "d4Opiha6GY3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a dictionary. You can copy and modify one from an NNSVS dic. Each line is a syllable, followed by an indentation, followed by the phonemes separated by a space. You can only have two phonemes in one entry. It should be in the following format: \\\n",
        "`a\ta \\\n",
        "ka\tk a \\\n",
        "sa\ts a` \\\n",
        "When you are done making your dictionary, upload it to /content/DiffSinger/dictionaries ."
      ],
      "metadata": {
        "id": "bZrtWAM0HHvC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown #Edit Config\n",
        "# @markdown  You will want to edit the following configurations:\n",
        "# @markdown   - speakers: replace {speaker_name} with the name of your dataset\n",
        "# @markdown   - test_prefixes: replace the numbers with some wav file names (without extensions) from your segments folder\n",
        "# @markdown   - raw_data_dir: replace {speaker_name} with the name of your singer\n",
        "# @markdown   - binary_data_dir: replace {speaker_name} with the name of your singer\n",
        "# @markdown   - dictionary: replace with the path to your dictionary\n",
        "%%writefile /content/DiffSinger/configs/acoustic.yaml\n",
        "base_config:\n",
        "  - configs/base.yaml\n",
        "\n",
        "task_cls: training.acoustic_task.AcousticTask\n",
        "num_spk: 1\n",
        "speakers:\n",
        "  - {speaker_name}\n",
        "spk_ids: []\n",
        "test_prefixes: [\n",
        "    '2044',\n",
        "    '2086',\n",
        "    '2092',\n",
        "    '2093',\n",
        "    '2100',\n",
        "]\n",
        "\n",
        "vocoder: NsfHifiGAN\n",
        "vocoder_ckpt: checkpoints/nsf_hifigan/model\n",
        "audio_sample_rate: 44100\n",
        "audio_num_mel_bins: 128\n",
        "hop_size: 512            # Hop size.\n",
        "fft_size: 2048           # FFT size.\n",
        "win_size: 2048           # FFT size.\n",
        "fmin: 40\n",
        "fmax: 16000\n",
        "\n",
        "binarization_args:\n",
        "  shuffle: true\n",
        "  num_workers: 0\n",
        "augmentation_args:\n",
        "  random_pitch_shifting:\n",
        "    enabled: false\n",
        "    range: [-5., 5.]\n",
        "    scale: 1.0\n",
        "  fixed_pitch_shifting:\n",
        "    enabled: false\n",
        "    targets: [-5., 5.]\n",
        "    scale: 0.75\n",
        "  random_time_stretching:\n",
        "    enabled: false\n",
        "    range: [0.5, 2.]\n",
        "    domain: log  # or linear\n",
        "    scale: 1.0\n",
        "\n",
        "raw_data_dir: 'data/{speaker_name}/segments'\n",
        "binary_data_dir: 'data/{speaker_name}/binary'\n",
        "binarizer_cls: preprocessing.acoustic_binarizer.AcousticBinarizer\n",
        "dictionary: dictionaries/opencpop-extension.txt\n",
        "num_pad_tokens: 1\n",
        "spec_min: [-5]\n",
        "spec_max: [0]\n",
        "mel_vmin: -6. #-6.\n",
        "mel_vmax: 1.5\n",
        "interp_uv: true\n",
        "energy_smooth_width: 0.12\n",
        "breathiness_smooth_width: 0.12\n",
        "\n",
        "use_spk_id: false\n",
        "f0_embed_type: continuous\n",
        "use_energy_embed: false\n",
        "use_breathiness_embed: false\n",
        "use_key_shift_embed: false\n",
        "use_speed_embed: false\n",
        "\n",
        "K_step: 1000\n",
        "timesteps: 1000\n",
        "max_beta: 0.02\n",
        "rel_pos: true\n",
        "diff_accelerator: ddim\n",
        "pndm_speedup: 10\n",
        "hidden_size: 256\n",
        "residual_layers: 20\n",
        "residual_channels: 512\n",
        "dilation_cycle_length: 4  # *\n",
        "diff_decoder_type: 'wavenet'\n",
        "diff_loss_type: l1\n",
        "schedule_type: 'linear'\n",
        "\n",
        "# train and eval\n",
        "num_sanity_val_steps: 1\n",
        "optimizer_args:\n",
        "  lr: 0.0004\n",
        "lr_scheduler_args:\n",
        "  step_size: 20000\n",
        "  gamma: 0.5\n",
        "max_batch_frames: 80000\n",
        "max_batch_size: 8\n",
        "val_with_vocoder: true\n",
        "val_check_interval: 2000\n",
        "num_valid_plots: 10\n",
        "max_updates: 320000\n",
        "num_ckpt_keep: 5\n",
        "permanent_ckpt_start: 200000\n",
        "permanent_ckpt_interval: 40000\n",
        "\n",
        "\n",
        "finetune_enabled: false\n",
        "finetune_ckpt_path: null\n",
        "\n",
        "finetune_ignored_params:\n",
        "  - model.fs2.encoder.embed_tokens\n",
        "  - model.fs2.txt_embed\n",
        "  - model.fs2.spk_embed\n",
        "finetune_strict_shapes: true\n",
        "\n",
        "freezing_enabled: false\n",
        "frozen_params: []"
      ],
      "metadata": {
        "id": "SoalO2K8DbgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Set output directory of checkpoints\n",
        "#@markdown It is recommended to set this to your Google Drive so you don't lose checkpoints if colab disconnects you.\n",
        "%%writefile /content/DiffSinger/utils/hparams.py\n",
        "output_directory = ''#@param{type:'string'}\n",
        "\n",
        "import argparse\n",
        "import multiprocessing\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "\n",
        "import yaml\n",
        "\n",
        "global_print_hparams = True\n",
        "hparams = {}\n",
        "\n",
        "\n",
        "class Args:\n",
        "    def __init__(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            self.__setattr__(k, v)\n",
        "\n",
        "\n",
        "def override_config(old_config: dict, new_config: dict):\n",
        "    for k, v in new_config.items():\n",
        "        if isinstance(v, dict) and k in old_config:\n",
        "            override_config(old_config[k], new_config[k])\n",
        "        else:\n",
        "            old_config[k] = v\n",
        "\n",
        "\n",
        "def set_hparams(config='', exp_name='', hparams_str='', print_hparams=True, global_hparams=True):\n",
        "    \"\"\"\n",
        "        Load hparams from multiple sources:\n",
        "        1. config chain (i.e. first load base_config, then load config);\n",
        "        2. if reset == True, load from the (auto-saved) complete config file ('config.yaml')\n",
        "           which contains all settings and do not rely on base_config;\n",
        "        3. load from argument --hparams or hparams_str, as temporary modification.\n",
        "    \"\"\"\n",
        "    if config == '':\n",
        "        parser = argparse.ArgumentParser(description='neural music')\n",
        "        parser.add_argument('--config', type=str, default='',\n",
        "                            help='location of the data corpus')\n",
        "        parser.add_argument('--exp_name', type=str, default='', help='exp_name')\n",
        "        parser.add_argument('--hparams', type=str, default='',\n",
        "                            help='location of the data corpus')\n",
        "        parser.add_argument('--infer', action='store_true', help='infer')\n",
        "        parser.add_argument('--validate', action='store_true', help='validate')\n",
        "        parser.add_argument('--reset', action='store_true', help='reset hparams')\n",
        "        parser.add_argument('--debug', action='store_true', help='debug')\n",
        "        args, unknown = parser.parse_known_args()\n",
        "    else:\n",
        "        args = Args(config=config, exp_name=exp_name, hparams=hparams_str,\n",
        "                    infer=False, validate=False, reset=False, debug=False)\n",
        "\n",
        "    args_work_dir = ''\n",
        "    if args.exp_name != '':\n",
        "        args.work_dir = args.exp_name\n",
        "        args_work_dir = output_directory\n",
        "\n",
        "    config_chains = []\n",
        "    loaded_config = set()\n",
        "\n",
        "    def load_config(config_fn):  # deep first\n",
        "        with open(config_fn, encoding='utf-8') as f:\n",
        "            hparams_ = yaml.safe_load(f)\n",
        "        loaded_config.add(config_fn)\n",
        "        if 'base_config' in hparams_:\n",
        "            ret_hparams = {}\n",
        "            if not isinstance(hparams_['base_config'], list):\n",
        "                hparams_['base_config'] = [hparams_['base_config']]\n",
        "            for c in hparams_['base_config']:\n",
        "                if c not in loaded_config:\n",
        "                    if c.startswith('.'):\n",
        "                        c = f'{os.path.dirname(config_fn)}/{c}'\n",
        "                        c = os.path.normpath(c)\n",
        "                    override_config(ret_hparams, load_config(c))\n",
        "            override_config(ret_hparams, hparams_)\n",
        "        else:\n",
        "            ret_hparams = hparams_\n",
        "        config_chains.append(config_fn)\n",
        "        return ret_hparams\n",
        "\n",
        "    global hparams\n",
        "    assert args.config != '' or args_work_dir != '', 'Either config or exp name should be specified.'\n",
        "    saved_hparams = {}\n",
        "    ckpt_config_path = f'{args_work_dir}/config.yaml'\n",
        "    if os.path.exists(ckpt_config_path):\n",
        "        with open(ckpt_config_path, encoding='utf-8') as f:\n",
        "            saved_hparams.update(yaml.safe_load(f))\n",
        "\n",
        "    hparams_ = {}\n",
        "    if args.config != '':\n",
        "        hparams_.update(load_config(args.config))\n",
        "\n",
        "    if not args.reset:\n",
        "        hparams_.update(saved_hparams)\n",
        "    hparams_['work_dir'] = args_work_dir\n",
        "\n",
        "    if args.hparams != \"\":\n",
        "        for new_hparam in args.hparams.split(\",\"):\n",
        "            k, v = new_hparam.split(\"=\")\n",
        "            if k not in hparams_:\n",
        "                hparams_[k] = eval(v)\n",
        "            if v in ['True', 'False'] or type(hparams_[k]) == bool:\n",
        "                hparams_[k] = eval(v)\n",
        "            else:\n",
        "                hparams_[k] = type(hparams_[k])(v)\n",
        "\n",
        "    dictionary = hparams_.get('g2p_dictionary')\n",
        "    if dictionary is None:\n",
        "        dictionary = 'dictionaries/opencpop.txt'\n",
        "    ckpt_dictionary = os.path.join(hparams_['work_dir'], os.path.basename(dictionary))\n",
        "    if args_work_dir != '' and (not os.path.exists(ckpt_config_path) or args.reset) and not args.infer:\n",
        "        os.makedirs(hparams_['work_dir'], exist_ok=True)\n",
        "        if not bool(re.match(r'Process-\\d+', multiprocessing.current_process().name)):\n",
        "            # Only the main process will save the config file and dictionary\n",
        "            with open(ckpt_config_path, 'w', encoding='utf-8') as f:\n",
        "                hparams_non_recursive = hparams_.copy()\n",
        "                hparams_non_recursive['base_config'] = []\n",
        "                yaml.safe_dump(hparams_non_recursive, f, allow_unicode=True, encoding='utf-8')\n",
        "            if hparams_.get('reset_phone_dict') or not os.path.exists(ckpt_dictionary):\n",
        "                shutil.copy(dictionary, ckpt_dictionary)\n",
        "\n",
        "    ckpt_dictionary_exists = os.path.exists(ckpt_dictionary)\n",
        "    if not os.path.exists(dictionary) and not ckpt_dictionary_exists:\n",
        "        raise FileNotFoundError(f'G2P dictionary not found in either of the following paths:\\n'\n",
        "                                f' - \\'{dictionary}\\'\\n'\n",
        "                                f' - \\'{ckpt_dictionary}\\'')\n",
        "    hparams_['original_g2p_dictionary'] = dictionary\n",
        "    if ckpt_dictionary_exists:\n",
        "        dictionary = ckpt_dictionary\n",
        "    hparams_['g2p_dictionary'] = dictionary\n",
        "\n",
        "    hparams_['infer'] = args.infer\n",
        "    hparams_['debug'] = args.debug\n",
        "    hparams_['validate'] = args.validate\n",
        "    global global_print_hparams\n",
        "    if global_hparams:\n",
        "        hparams.clear()\n",
        "        hparams.update(hparams_)\n",
        "\n",
        "    if print_hparams and global_print_hparams and global_hparams:\n",
        "        print('| Hparams chains: ', config_chains)\n",
        "        print('| Hparams: ')\n",
        "        for i, (k, v) in enumerate(sorted(hparams_.items())):\n",
        "            print(f\"\\033[;33;m{k}\\033[0m: {v}, \", end=\"\\n\" if i % 5 == 4 else \"\")\n",
        "        print(\"\")\n",
        "        global_print_hparams = False\n",
        "    # print(hparams_.keys())\n",
        "    if hparams.get('exp_name') is None:\n",
        "        hparams['exp_name'] = args.exp_name\n",
        "    if hparams_.get('exp_name') is None:\n",
        "        hparams_['exp_name'] = args.exp_name\n",
        "    return hparams_"
      ],
      "metadata": {
        "cellView": "form",
        "id": "L9-K4zuI4S04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown # Migrate to new format\n",
        "# @markdown Input the path of your \"transcription.txt\". \\\n",
        "# @markdown This should generate a \"transcription.csv\" in the same folder.\n",
        "transcription_txt_path = ''#@param{type:'string'}\n",
        "!python scripts/migrate.py txt {transcription_txt_path}"
      ],
      "metadata": {
        "id": "J5J1j75vj4YV",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Preprocess data\n",
        "#@markdown Note: If you get a BinarizationError \"transcriptions and dictionary mismatch\", please ensure your data contains all phonemes matching the dictionary (and vice versa).\n",
        "import os\n",
        "os.environ['PYTHONPATH']='.'\n",
        "!CUDA_VISIBLE_DEVICES=0 python scripts/binarize.py --config configs/acoustic.yaml"
      ],
      "metadata": {
        "id": "Qkl7M918Jv8D",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Train your model\n",
        "#@markdown Enter the name of your singer.\n",
        "name = ''#@param{type:'string'}\n",
        "python scripts/train.py --config configs/acoustic.yaml --exp_name {name} --reset"
      ],
      "metadata": {
        "cellView": "form",
        "id": "zB9m1O9LKKzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Convert to ONNX for OpenUtau\n",
        "#@markdown Enter the path of the folder which contains your checkpoint into the `checkpoints_path` box, and the name of the folder into the `name` box. \\\n",
        "#@markdown It will be saved in the output path you specify.\n",
        "checkpoints_path = ''#@param{type:'string'}\n",
        "name = ''#@param{type:'string'}\n",
        "output_path = ''#@param{type:'string'}\n",
        "!cp {checkpoints_path} -r /content/DiffSinger/checkpoints\n",
        "!python scripts/export.py acoustic --exp {name} --out {output_path}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Zzy4-feGKnbU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}